{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Part 5: Sentiment Classification with Deep Learning using Keras\n",
        "In this final notebook, we cover the implementation of advanced techniques for sentiment analysis using modern techniques in deep learning. The implementation of these techniques may often require longer training periods but can often produce better classification results if the right architecture and features are used.\n",
        "\n",
        "This notebook covers the following:\n",
        "\n",
        "#### Dense Layer Neural Network for Sentiment Classification\n",
        "1. Implementation of Dense Layer in Keras\n",
        "2. Activation and Optimization metric set up\n",
        "3. Model Metric Specification\n",
        "\n",
        "#### Recurrent Neural Networks for Sentiment Classification\n",
        "1. Embedding layers with Keras\n",
        "2. Recurrent Neural Network Architecture\n",
        "3. Keras Model Implementation and Parameter Setting"
      ],
      "metadata": {
        "id": "uuRFuakLctGD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Data Preparation\n",
        "Before we implement any of the above, we need to perform the following operations on the review dataset.\n",
        "\n",
        "1. Text Processing - Cleaning, Removing Punctuations and Numeric values\n",
        "2. Build a word2vector model - Using Word vectors for feature generation\n",
        "3. Averaging Word Vectors across review/text\n",
        "\n",
        "\n",
        "Let's begin"
      ],
      "metadata": {
        "id": "Rx0puEIBc0QG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        " \n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "EGOtoQ2rcyKE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.1. Preprocessing Text\n",
        "\n",
        "The function below implements basic preprocessing steps to clean the text ahead of developing word vectors"
      ],
      "metadata": {
        "id": "zrtHHhQoc6ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def processText(text):\n",
        "  \"\"\" Cleaning Function \"\"\"\n",
        "  text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "  text = re.sub('[0-9]+', '', text)\n",
        "  text = [word for word in text_to_word_sequence(text) ]\n",
        "  return \" \".join(text)"
      ],
      "metadata": {
        "id": "O96Mfox5c4o4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_data = pd.read_csv('restaurant_reviews.tsv', delimiter='\\t')\n",
        "review_data['clean_review'] = review_data.Review.apply( lambda x: processText(str(x)) )\n",
        "review_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "-pMRnr1Uc9G_",
        "outputId": "2373aab3-3c59-47f9-acb1-769adebcdbfb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              Review  Liked  \\\n",
              "0                           Wow... Loved this place.      1   \n",
              "1                                 Crust is not good.      0   \n",
              "2          Not tasty and the texture was just nasty.      0   \n",
              "3  Stopped by during the late May bank holiday of...      1   \n",
              "4  The selection on the menu was great and so wer...      1   \n",
              "\n",
              "                                        clean_review  \n",
              "0                               wow loved this place  \n",
              "1                                  crust is not good  \n",
              "2           not tasty and the texture was just nasty  \n",
              "3  stopped by during the late may bank holiday of...  \n",
              "4  the selection on the menu was great and so wer...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3382340f-bb09-425d-a99b-6bd7a930019a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>Liked</th>\n",
              "      <th>clean_review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "      <td>wow loved this place</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "      <td>crust is not good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "      <td>not tasty and the texture was just nasty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "      <td>stopped by during the late may bank holiday of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "      <td>the selection on the menu was great and so wer...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3382340f-bb09-425d-a99b-6bd7a930019a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3382340f-bb09-425d-a99b-6bd7a930019a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3382340f-bb09-425d-a99b-6bd7a930019a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.2 . Building Word2Vec Model: CBOW\n",
        "\n",
        "The implementation below develops a CBOW word-to-vector model of vector size 500 and window size 150."
      ],
      "metadata": {
        "id": "8BzbAUVWdCGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size = 500\n",
        "window_size = 150\n",
        "  \n",
        "corpus = [text_to_word_sequence(review) for review in review_data.clean_review.values]\n",
        "cbow_model = Word2Vec( sentences= corpus, vector_size = vector_size, window = window_size, sg=0,  min_count = 2, sample=.000001 )"
      ],
      "metadata": {
        "id": "a_qgtyXec-U1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.3. Averaging Word Vectors\n",
        "The last step is to average the word vectors in the CBOW model so that we can have a single vector that summarizes each individual vector in the sentiment."
      ],
      "metadata": {
        "id": "KVOUIzCVdLFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def avg_words_vectors(words, model, vocabulary , num_features ):\n",
        "    \"\"\"  \"\"\"\n",
        "    feature_vector = np.zeros((num_features,), dtype='float64')\n",
        "    word_count = 0\n",
        "    \n",
        "    for word in words:\n",
        "        if word in vocabulary:\n",
        "            word_count += 1\n",
        "            feature_vector = np.add(feature_vector, model.wv.get_vector(word))\n",
        "    \n",
        "    if word_count:\n",
        "        feature_vector = np.divide(feature_vector, word_count)\n",
        "    \n",
        "    return feature_vector\n",
        "\n",
        "def word_vectorizer(corpus, model, num_features):\n",
        "    \"\"\" Average Word Vectors \"\"\"\n",
        "    vocabulary = list(model.wv.index_to_key)\n",
        "    \n",
        "    features = [ avg_words_vectors(sentence, model, vocabulary, num_features) for sentence in corpus ]   \n",
        "    return np.array(features)"
      ],
      "metadata": {
        "id": "4kjPsJjSdIhy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_features = word_vectorizer(corpus=corpus, model=cbow_model, num_features=500)\n",
        "text_features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbGkK1bqdOAI",
        "outputId": "d01b22b1-a67c-425a-b3b9-1f6e4d780d18"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Classification with Dense Neural Network\n",
        "In this example, we will implement a simple fully connected Neural Network for classification. The neural network will have 2-3 Dense/Fully connected layers with drop-out layers to avoid overfitting. The output layer will be a binary output.\n",
        "\n",
        "Initial parameter for the model:\n",
        "\n",
        "1. batch_size = 50\n",
        "2. Training Epochs = 20\n",
        "3. Input_size/Vector Size = 500"
      ],
      "metadata": {
        "id": "W89_v7CSdRbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "# Dense Layer Architecture\n",
        "fully_connected_nn = Sequential()\n",
        "fully_connected_nn.add(Dense(200, activation='relu', input_shape=(vector_size, )))\n",
        "fully_connected_nn.add(Dropout(.5))\n",
        "fully_connected_nn.add(Dense(200, activation='relu'))\n",
        "fully_connected_nn.add(Dropout(.5))\n",
        "fully_connected_nn.add(Dense(200, activation='relu'))\n",
        "fully_connected_nn.add(Dropout(.5))\n",
        "fully_connected_nn.add(Dense(2, activation='softmax'))"
      ],
      "metadata": {
        "id": "R4T86Dw1dPbJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fully_connected_nn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Kzk0is8dUlh",
        "outputId": "fb69f2b8-e2db-4677-a037-8cd9f8e3d01c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 200)               100200    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 200)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 200)               40200     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 200)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 200)               40200     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 200)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 402       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 181,002\n",
            "Trainable params: 181,002\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to compile the model before training it on the dataset."
      ],
      "metadata": {
        "id": "TyYzH_uxdYeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fully_connected_nn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "x8YVeX5KdWMp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Training/Fitting the Model\n",
        "\n",
        "To train the model, we pass on features and targets, train and test split (as test data percentage), epochs for training, and batch sizes.\n",
        "\n"
      ],
      "metadata": {
        "id": "dxgV9Zn-dcbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target = to_categorical(review_data.Liked)\n",
        "\n",
        "fully_connected_nn.fit(text_features, target , validation_split=.3, shuffle=True, epochs=50, batch_size=150, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HObASlw7daik",
        "outputId": "119238c1-cfcd-45bb-f087-5a316b819e73"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "5/5 [==============================] - 4s 351ms/step - loss: 0.6918 - accuracy: 0.5471 - val_loss: 0.7025 - val_accuracy: 0.3600\n",
            "Epoch 2/50\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.6890 - accuracy: 0.5600 - val_loss: 0.7157 - val_accuracy: 0.3600\n",
            "Epoch 3/50\n",
            "5/5 [==============================] - 0s 59ms/step - loss: 0.6873 - accuracy: 0.5600 - val_loss: 0.7262 - val_accuracy: 0.3600\n",
            "Epoch 4/50\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.6857 - accuracy: 0.5600 - val_loss: 0.7351 - val_accuracy: 0.3600\n",
            "Epoch 5/50\n",
            "5/5 [==============================] - 0s 41ms/step - loss: 0.6853 - accuracy: 0.5600 - val_loss: 0.7408 - val_accuracy: 0.3600\n",
            "Epoch 6/50\n",
            "5/5 [==============================] - 0s 55ms/step - loss: 0.6872 - accuracy: 0.5600 - val_loss: 0.7393 - val_accuracy: 0.3600\n",
            "Epoch 7/50\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.6851 - accuracy: 0.5600 - val_loss: 0.7410 - val_accuracy: 0.3600\n",
            "Epoch 8/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.6858 - accuracy: 0.5600 - val_loss: 0.7364 - val_accuracy: 0.3600\n",
            "Epoch 9/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.6848 - accuracy: 0.5600 - val_loss: 0.7372 - val_accuracy: 0.3600\n",
            "Epoch 10/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.6836 - accuracy: 0.5600 - val_loss: 0.7356 - val_accuracy: 0.3600\n",
            "Epoch 11/50\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.6859 - accuracy: 0.5600 - val_loss: 0.7324 - val_accuracy: 0.3600\n",
            "Epoch 12/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.6853 - accuracy: 0.5600 - val_loss: 0.7296 - val_accuracy: 0.3600\n",
            "Epoch 13/50\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6859 - accuracy: 0.5600 - val_loss: 0.7277 - val_accuracy: 0.3600\n",
            "Epoch 14/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.6820 - accuracy: 0.5600 - val_loss: 0.7331 - val_accuracy: 0.3600\n",
            "Epoch 15/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.6832 - accuracy: 0.5600 - val_loss: 0.7427 - val_accuracy: 0.3600\n",
            "Epoch 16/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6850 - accuracy: 0.5600 - val_loss: 0.7414 - val_accuracy: 0.3600\n",
            "Epoch 17/50\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6830 - accuracy: 0.5600 - val_loss: 0.7289 - val_accuracy: 0.3600\n",
            "Epoch 18/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6821 - accuracy: 0.5600 - val_loss: 0.7283 - val_accuracy: 0.3600\n",
            "Epoch 19/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.6807 - accuracy: 0.5600 - val_loss: 0.7266 - val_accuracy: 0.3600\n",
            "Epoch 20/50\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.6784 - accuracy: 0.5600 - val_loss: 0.7432 - val_accuracy: 0.3600\n",
            "Epoch 21/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.6744 - accuracy: 0.5600 - val_loss: 0.7206 - val_accuracy: 0.3600\n",
            "Epoch 22/50\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.6700 - accuracy: 0.5614 - val_loss: 0.7178 - val_accuracy: 0.3600\n",
            "Epoch 23/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.6643 - accuracy: 0.5629 - val_loss: 0.7208 - val_accuracy: 0.3600\n",
            "Epoch 24/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.6544 - accuracy: 0.5743 - val_loss: 0.7144 - val_accuracy: 0.3900\n",
            "Epoch 25/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6393 - accuracy: 0.6000 - val_loss: 0.7118 - val_accuracy: 0.4333\n",
            "Epoch 26/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6309 - accuracy: 0.6243 - val_loss: 0.6718 - val_accuracy: 0.5900\n",
            "Epoch 27/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.6245 - accuracy: 0.6871 - val_loss: 0.6961 - val_accuracy: 0.5033\n",
            "Epoch 28/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5998 - accuracy: 0.6700 - val_loss: 0.6546 - val_accuracy: 0.6133\n",
            "Epoch 29/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.5775 - accuracy: 0.6886 - val_loss: 0.6761 - val_accuracy: 0.5767\n",
            "Epoch 30/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5723 - accuracy: 0.7014 - val_loss: 0.6576 - val_accuracy: 0.6133\n",
            "Epoch 31/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.5385 - accuracy: 0.7371 - val_loss: 0.6778 - val_accuracy: 0.5867\n",
            "Epoch 32/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5408 - accuracy: 0.7429 - val_loss: 0.6046 - val_accuracy: 0.7000\n",
            "Epoch 33/50\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.5263 - accuracy: 0.7300 - val_loss: 0.5673 - val_accuracy: 0.7233\n",
            "Epoch 34/50\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5094 - accuracy: 0.7529 - val_loss: 0.7783 - val_accuracy: 0.5300\n",
            "Epoch 35/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.5042 - accuracy: 0.7500 - val_loss: 0.5516 - val_accuracy: 0.7333\n",
            "Epoch 36/50\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5089 - accuracy: 0.7600 - val_loss: 0.6074 - val_accuracy: 0.6900\n",
            "Epoch 37/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.4891 - accuracy: 0.7700 - val_loss: 0.6540 - val_accuracy: 0.6333\n",
            "Epoch 38/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.4632 - accuracy: 0.7743 - val_loss: 0.5465 - val_accuracy: 0.7433\n",
            "Epoch 39/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.4524 - accuracy: 0.7943 - val_loss: 0.6116 - val_accuracy: 0.6867\n",
            "Epoch 40/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.4365 - accuracy: 0.8100 - val_loss: 0.5817 - val_accuracy: 0.7300\n",
            "Epoch 41/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.4361 - accuracy: 0.8043 - val_loss: 0.6150 - val_accuracy: 0.6900\n",
            "Epoch 42/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.4285 - accuracy: 0.8129 - val_loss: 0.7266 - val_accuracy: 0.6067\n",
            "Epoch 43/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.4604 - accuracy: 0.7729 - val_loss: 0.5390 - val_accuracy: 0.7400\n",
            "Epoch 44/50\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.4296 - accuracy: 0.8100 - val_loss: 0.5417 - val_accuracy: 0.7400\n",
            "Epoch 45/50\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.4022 - accuracy: 0.8214 - val_loss: 0.6501 - val_accuracy: 0.6700\n",
            "Epoch 46/50\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.4150 - accuracy: 0.8129 - val_loss: 0.5384 - val_accuracy: 0.7500\n",
            "Epoch 47/50\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.3973 - accuracy: 0.8286 - val_loss: 0.5433 - val_accuracy: 0.7433\n",
            "Epoch 48/50\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.3914 - accuracy: 0.8300 - val_loss: 0.6254 - val_accuracy: 0.7100\n",
            "Epoch 49/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.3741 - accuracy: 0.8514 - val_loss: 0.5757 - val_accuracy: 0.7300\n",
            "Epoch 50/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.3767 - accuracy: 0.8200 - val_loss: 0.5213 - val_accuracy: 0.7733\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa34c8f3100>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Model Performance and Improvements\n",
        "The trained model has an 89% accuracy on the train set and 76% accuracy on the test set which is very good performance for a simple dense model.\n",
        "\n",
        "<br>\n",
        "Notice that the validation loss is increasing as the training loss is decreasing which suggests overfitting. To improve performance, we can introduce more regularizers and change the architecture. For our purposes, this is sufficient. We move on to using RNNs."
      ],
      "metadata": {
        "id": "QKP8ijAtdjbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Embedding Layers and Recurrent Neural Networks\n",
        "Word Embeddings are a little different from the averaged word vector features we used in our deep layer neural network because they use word indexing in place for the words in the corpus and assign an indeces to the words to represent their presence or lack of in a particular document.\n",
        "\n",
        "<br>\n",
        "Let's demonstrate this in practice. We can first count the frequency of the words below and use the variable to determine the number of unique words in the corpus"
      ],
      "metadata": {
        "id": "Q2tyU3NRdnsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "token_counter = Counter([token for review in corpus for token in review])"
      ],
      "metadata": {
        "id": "IuQy64ptdf2H"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use the token counter to create a dictionary that indexes all the words to some unique value."
      ],
      "metadata": {
        "id": "IxfqNZUEds8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_map = { item[0]:index+1 for index,item in enumerate(dict(token_counter).items()) } # Index all the words starting at 1\n",
        "max_index = np.max(list(vocab_map.values()))   "
      ],
      "metadata": {
        "id": "xbmOFtRIdrcO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that we may have instances when loading new text that the words are not in the vocabulary map. In the code below, we add two things: a padding index value matched to index 0 and not found index as the last words to catch those two scenarios."
      ],
      "metadata": {
        "id": "gREBKtuQdydL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_map['PAD_INDEX'] = 0\n",
        "vocab_map['NOT_FOUND_INDEX'] = max_index + 1\n",
        "vocab_size = len(vocab_map)"
      ],
      "metadata": {
        "id": "TXMH66I-dx7O"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to obtain the maximum length of the review. We use the following list comprehension to return the maximum review."
      ],
      "metadata": {
        "id": "fWJY_0smd3h2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = np.max([len(review) for review in corpus])\n",
        "max_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s53o3Gmd1wH",
        "outputId": "3f65a8a2-de06-4ccd-abd7-8bce672cde1f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Padding Text Sequences\n",
        "\n",
        "We need to pad the text sequences to make sure that all reviews have the same input length. To do this, we use the keras padding method."
      ],
      "metadata": {
        "id": "a2F8pCeBd7ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "padding_reviews = [[vocab_map[token] for token in review] for review in corpus]\n",
        "input_features = pad_sequences(padding_reviews, max_len)\n",
        "\n",
        "input_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wehI7beYd5KQ",
        "outputId": "dde7d116-b8ab-4353-ceee-62b3b7481aff"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,    2,    3,    4],\n",
              "       [   0,    0,    0, ...,    6,    7,    8],\n",
              "       [   0,    0,    0, ...,   13,   14,   15],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,    7,   76,   77],\n",
              "       [   0,    0,    0, ...,  516,  512,   63],\n",
              "       [   0,    0,    0, ..., 1323,   11,  528]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  2.2. Building a Recurrent Neural Network\n",
        "\n",
        "The RNN will be a relatively simple network with one LSTM layer, an Embedding layer, and a dropout layer for regularization. See the architecture below."
      ],
      "metadata": {
        "id": "CDoceR6BeBGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Dropout, Flatten\n",
        "from keras.layers import LSTM\n",
        "\n",
        "Embedding_dim = 128\n",
        "LSTM_DIM = 64"
      ],
      "metadata": {
        "id": "sH4yhm1fd-x7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model = Sequential()\n",
        "rnn_model.add(Embedding(input_dim=vocab_size, output_dim=Embedding_dim, input_length=max_len))\n",
        "rnn_model.add(Dropout(.2))\n",
        "rnn_model.add(LSTM(LSTM_DIM, dropout=.2, recurrent_dropout=.2))\n",
        "rnn_model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "rnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "zNjAEtOXeEeH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Fittting the RNN Classifier\n",
        "In fitting our RNN we will use the following hyperparameters:\n",
        "\n",
        "1. batch_size = 100\n",
        "2. Epochs = 20"
      ],
      "metadata": {
        "id": "GUrKHXhUeIUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model.fit(input_features, target, epochs=20, batch_size=100, shuffle=True, validation_split=.3, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysKAmCEfeGW8",
        "outputId": "d3316579-75c8-444d-e336-1021fbebcb68"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "7/7 [==============================] - 5s 160ms/step - loss: 0.6890 - accuracy: 0.5500 - val_loss: 0.7065 - val_accuracy: 0.3600\n",
            "Epoch 2/20\n",
            "7/7 [==============================] - 1s 101ms/step - loss: 0.6754 - accuracy: 0.5600 - val_loss: 0.7368 - val_accuracy: 0.3600\n",
            "Epoch 3/20\n",
            "7/7 [==============================] - 1s 158ms/step - loss: 0.6639 - accuracy: 0.5614 - val_loss: 0.7440 - val_accuracy: 0.3600\n",
            "Epoch 4/20\n",
            "7/7 [==============================] - 1s 188ms/step - loss: 0.6356 - accuracy: 0.6171 - val_loss: 0.7030 - val_accuracy: 0.4033\n",
            "Epoch 5/20\n",
            "7/7 [==============================] - 1s 150ms/step - loss: 0.5927 - accuracy: 0.7171 - val_loss: 0.7067 - val_accuracy: 0.4467\n",
            "Epoch 6/20\n",
            "7/7 [==============================] - 1s 104ms/step - loss: 0.5086 - accuracy: 0.7514 - val_loss: 0.6620 - val_accuracy: 0.5767\n",
            "Epoch 7/20\n",
            "7/7 [==============================] - 1s 101ms/step - loss: 0.3968 - accuracy: 0.8586 - val_loss: 0.6379 - val_accuracy: 0.6833\n",
            "Epoch 8/20\n",
            "7/7 [==============================] - 1s 103ms/step - loss: 0.3222 - accuracy: 0.9386 - val_loss: 0.6456 - val_accuracy: 0.6833\n",
            "Epoch 9/20\n",
            "7/7 [==============================] - 1s 101ms/step - loss: 0.2587 - accuracy: 0.9200 - val_loss: 0.6584 - val_accuracy: 0.6933\n",
            "Epoch 10/20\n",
            "7/7 [==============================] - 1s 99ms/step - loss: 0.1975 - accuracy: 0.9586 - val_loss: 0.5613 - val_accuracy: 0.7267\n",
            "Epoch 11/20\n",
            "7/7 [==============================] - 1s 100ms/step - loss: 0.1507 - accuracy: 0.9729 - val_loss: 0.6780 - val_accuracy: 0.7333\n",
            "Epoch 12/20\n",
            "7/7 [==============================] - 1s 99ms/step - loss: 0.1045 - accuracy: 0.9714 - val_loss: 0.6667 - val_accuracy: 0.7433\n",
            "Epoch 13/20\n",
            "7/7 [==============================] - 1s 103ms/step - loss: 0.0839 - accuracy: 0.9800 - val_loss: 0.6781 - val_accuracy: 0.7333\n",
            "Epoch 14/20\n",
            "7/7 [==============================] - 1s 97ms/step - loss: 0.0659 - accuracy: 0.9857 - val_loss: 0.7805 - val_accuracy: 0.7233\n",
            "Epoch 15/20\n",
            "7/7 [==============================] - 1s 99ms/step - loss: 0.0516 - accuracy: 0.9929 - val_loss: 0.8447 - val_accuracy: 0.7400\n",
            "Epoch 16/20\n",
            "7/7 [==============================] - 1s 107ms/step - loss: 0.0466 - accuracy: 0.9886 - val_loss: 0.7458 - val_accuracy: 0.7367\n",
            "Epoch 17/20\n",
            "7/7 [==============================] - 1s 105ms/step - loss: 0.0405 - accuracy: 0.9971 - val_loss: 0.7937 - val_accuracy: 0.7400\n",
            "Epoch 18/20\n",
            "7/7 [==============================] - 1s 100ms/step - loss: 0.0350 - accuracy: 0.9957 - val_loss: 0.8528 - val_accuracy: 0.7533\n",
            "Epoch 19/20\n",
            "7/7 [==============================] - 1s 147ms/step - loss: 0.0297 - accuracy: 0.9957 - val_loss: 0.9344 - val_accuracy: 0.7733\n",
            "Epoch 20/20\n",
            "7/7 [==============================] - 2s 263ms/step - loss: 0.0292 - accuracy: 0.9986 - val_loss: 0.8170 - val_accuracy: 0.7533\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa34cf826b0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that over time the accuracy increases in the training set to 99% but the validation loss decreases and then increases indicating overfitting. It is also important to note that the dataset we are using is rather small with only about 1000 textual reviews for which neural networks can easily overfit.\n",
        "\n"
      ],
      "metadata": {
        "id": "ifea5-eoePl6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicting Sentiment of New Reviews\n",
        "\n",
        "We have built our model and wish to deploy it in new reviews to see the performance. We will need to preprocess the new input in the same way we preprocessed the training dataset. Let's see this in action below."
      ],
      "metadata": {
        "id": "2oK27lW9eRZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = \"the food was great!\"\n",
        "\n",
        "new_text = [[vocab_map[token] for token in text_to_word_sequence(new_text)]]\n",
        "padded_text_input = pad_sequences(new_text, max_len)\n",
        "padded_text_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV67UcLceQvN",
        "outputId": "489dd4bd-9f8b-4c6f-f511-3ba82522a38d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  11, 124,  13,  31]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model.predict(padded_text_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxASC1fqeWs1",
        "outputId": "7adf6cf7-a43b-4ed9-b891-fcf723150b27"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 516ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.01302621, 0.98697376]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that our model has predicted 98.7% probability of a positive review and abour 1.3% probability that our sentiment is a negative review. Not bad for test prediction.\n",
        "\n",
        "<br>\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "In the series, we covered a number of NLP techniques from traditional feature extraction techniques like TFIDF and Bag of Words to modern techniques like Word2Vect and Embeddings. We have learned how to use word embedding and word vectors to build classification and sentiment analysis models. Try using the above model architectures and techniques to build a model for the Amazon product reviews data and set the results. Deep Learning techniques offer a lot of opportunities to improve model performance with additional layers, regularizers, and hyperparameter changes!"
      ],
      "metadata": {
        "id": "jMpWXrjseiKw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7b0Wj45Nep1A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}