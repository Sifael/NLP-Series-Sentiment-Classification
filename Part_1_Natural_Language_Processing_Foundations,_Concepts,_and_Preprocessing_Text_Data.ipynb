{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Natural Language Processing Foundations, Concepts, and Preprocessing Text Data\n",
        "\n",
        "This 5-part series covers concepts, analysis, and Machine Learning Models for Natural Language Processing. It was part of a set of lectures I gave at Emory University. It is intended to provide a practical introduction to NLP with Python, covering fundamental NLP terminology, tools, and data processing. In this note, I will cover the following concepts:\n",
        "<br>\n",
        "\n",
        "\n",
        "#### Formal Concepts and Definitions\n",
        "1. Corpus\n",
        "2. Documents\n",
        "3. Tokens\n",
        "\n",
        "\n",
        "#### General Text Preprocessing Techniques\n",
        "1. Tokenization: Word, Sentence, Character\n",
        "2. Stemming and Lemmatization\n",
        "3. Stop words and how to deal with them\n",
        "4. Punctuation and how to deal with them\n",
        "\n",
        "\n",
        "#### Introducing NLTK for Text Preprocessing\n",
        "1. Combining preprocessing steps\n",
        "2. Implementing preprocessing steps\n",
        "\n"
      ],
      "metadata": {
        "id": "cx-EmWx2Qf1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset: Amazon Product Reviews\n",
        "\n",
        "The dataset used in this series is for Amazon Product reviews. It is available at: Amazon Product Reviews Github\n",
        "\n",
        "<br>\n",
        "To begin, download the dataset from GitHub and save it in your working directory and use the following code to read in the dataset"
      ],
      "metadata": {
        "id": "xxzpXmBQQtca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "y-SAfZQJQsTT",
        "outputId": "dadbeeba-629e-4ecc-b8b1-d3e3951e9c53"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.5.3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_data = pd.read_csv('amazon_baby_review.csv')\n",
        "review_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "EifKrRtTQ2at",
        "outputId": "dd4bfcea-754d-4d75-a1fb-c25848525fea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  rating  sentiment\n",
              "0  All of my kids have cried non-stop when I trie...       5          1\n",
              "1  We wanted to get something to keep track of ou...       5          1\n",
              "2  My daughter had her 1st baby over a year ago. ...       5          1\n",
              "3  One of baby's first and favorite books, and it...       4          1\n",
              "4  Very cute interactive book! My son loves this ...       5          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-40b04fc9-55b2-442d-ad00-537b2fbecfa4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>rating</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>All of my kids have cried non-stop when I trie...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>We wanted to get something to keep track of ou...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>My daughter had her 1st baby over a year ago. ...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>One of baby's first and favorite books, and it...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Very cute interactive book! My son loves this ...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-40b04fc9-55b2-442d-ad00-537b2fbecfa4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-40b04fc9-55b2-442d-ad00-537b2fbecfa4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-40b04fc9-55b2-442d-ad00-537b2fbecfa4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our dataset, let's take a quick look at what we're working with. The dataset consists of three columns with the description below: \n",
        "<br>\n",
        "\n",
        " 1. review: Text review on a baby product\n",
        " 2. rating: Star rating associated with sentiment 1-5 with 3 removed\n",
        " 3. sentiment: Dummy variable for sentiment: 1: positive, -1: negative"
      ],
      "metadata": {
        "id": "L-OEMTtMR0Mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(review_data.rating.value_counts(normalize=True))\n",
        "print(review_data.sentiment.value_counts(normalize=True)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqTXNdg1Rybv",
        "outputId": "6f14e6de-3d2a-4553-eeec-e8021ba3a7e7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5    0.381953\n",
            "1    0.286083\n",
            "2    0.213107\n",
            "4    0.118857\n",
            "Name: rating, dtype: float64\n",
            " 1    0.50081\n",
            "-1    0.49919\n",
            "Name: sentiment, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall we have a 50-50 split of negative and positive reviews. Now let's get into the natural language processing concepts."
      ],
      "metadata": {
        "id": "M8SjhqizSBxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Corpus\n",
        "\n",
        "In the field of Natural Language Processing, the term 'corpus' refers to a body of text, which is a collection of text data utilized for analytical or machine learning purposes. In our specific case, the corpus consists of 53,072 reviews."
      ],
      "metadata": {
        "id": "diGGAKo0SD9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = review_data.review.tolist()\n",
        "  \n",
        "print(\"The size of our corpus is:\", len(corpus), \"\\n\")\n",
        "print(\"The first Review in our corpus is:\\n\", corpus[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDNKaeNMR-JH",
        "outputId": "b9af1e4b-ad94-4940-f5fb-0bfbc5867e9f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The size of our corpus is: 53072 \n",
            "\n",
            "The first Review in our corpus is:\n",
            " All of my kids have cried non-stop when I tried to ween them off their pacifier, until I found Thumbuddy To Love's Binky Fairy Puppet.  It is an easy way to work with your kids to allow them to understand where their pacifier is going and help them part from it.This is a must buy book, and a great gift for expecting parents!!  You will save them soo many headaches.Thanks for this book!  You all rock!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Documents\n",
        "\n",
        "Documents are individual entities that together make up a corpus. In our case, every review in our dataset is a document. That is we have 50,000 thousand documents in our corpus of Amazon product reviews. Here is a sample document."
      ],
      "metadata": {
        "id": "TrDxLarpSJvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_document = corpus[6]\n",
        "sample_document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vrUH5UrLSHIS",
        "outputId": "8a253803-2c21-40ef-dba4-85a142a44831"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Try this out for a spring project !Easy ,fun and affordable wall decals ...Fine quality and brightens up any room.. 5+ **********'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Tokens\n",
        "A token is a meaningful entity that makes up a document, similar to how words make up sentences. The choice of tokens can vary depending on the task and corpus being analyzed; they can encompass sentences, phrases, words, or even individual characters. In many NLP applications, words serve as the fundamental tokens.\n",
        "\n"
      ],
      "metadata": {
        "id": "w91HOge6SOue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### General Text Preprocessing Techniques\n",
        "\n",
        "Preprocessing refers to a set of operations applied to the body of text (corpus) to enable meaningful extraction of features and insights on the data. Similar to transformations in numerical data, text data needs to be preprocessed to determine things like the size of vocabulary, correction of misspellings, removing unnecessary text that have no useful features for the required tasks, and so forth. One of the primary ways of preprocessing data is to separate them into meaningful entities through tokenization"
      ],
      "metadata": {
        "id": "PDO6X2JpSRPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Tokenization\n",
        "Tokenization is the process of dividing documents individual meaningful tokens that make up the document. For example, a restaurant review may be tokenized sentences as the most meaningful tokens. They could also be tokenized into words, even letters.\n",
        "\n",
        "Let's see an example of tokenizers .\n",
        "\n",
        "### 1.1. Sentence Tokenization\n",
        "\n",
        "Sentence Tokenizers divide a paragraph of text into individual sentences that make the paragraph. Typically, sentence tokenizers use heuristics from sentence structure to split a paragrapn into sentences.\n",
        "\n",
        "<br>\n",
        "nltk is a wonderful package that provides tokenization tools that perform very well. Let's use the sent_tokenize method to tokenize our first document"
      ],
      "metadata": {
        "id": "Jw7UmY0cSZVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "print('Original Review:')\n",
        "review_data.review[11]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "swcoApK4SMcF",
        "outputId": "7010de70-c6b0-4714-8818-f3a7e1312c4e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Review:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm a new mom and I was looking for something to record my baby's daily activities and patterns and it is perfect! It has a lot of space to write down anything extra beyond the diaper changes, feedings and sleeping. I like that it has an area to write down medications. It is a great reminder for dr appointments and to track your baby's patterns. I am extremely happy with this book and will be ordering another one for when I run out of pages in 3 months.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Sentence Tokenized:')\n",
        "sent_tokenize(review_data.review[11])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkNOReCqShDc",
        "outputId": "39ad99b7-bb84-4b66-b1fe-59dcf3d45ac8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokenized:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I'm a new mom and I was looking for something to record my baby's daily activities and patterns and it is perfect!\",\n",
              " 'It has a lot of space to write down anything extra beyond the diaper changes, feedings and sleeping.',\n",
              " 'I like that it has an area to write down medications.',\n",
              " \"It is a great reminder for dr appointments and to track your baby's patterns.\",\n",
              " 'I am extremely happy with this book and will be ordering another one for when I run out of pages in 3 months.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the resulting output is a list of sentences. The sent_tokenize uses heuristic rules about the English language to separate sentences. In this case, look for periods and punctuation that separate complete sentences i.e. !"
      ],
      "metadata": {
        "id": "GGPKhMRhSnxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Punckt Sentence Tokenizer\n",
        "\n",
        "Another sentence tokenizer is the PunktSentenceTokenizer which performs similarly and may be more effective in some instances."
      ],
      "metadata": {
        "id": "eHF098inSy6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "punkt_sent_tokenizer = PunktSentenceTokenizer()\n",
        "punkt_sent_tokenizer.tokenize( review_data.review[11])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCfI40RoSqb0",
        "outputId": "e8786398-b132-4af5-a56f-e5416a8ec601"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I'm a new mom and I was looking for something to record my baby's daily activities and patterns and it is perfect!\",\n",
              " 'It has a lot of space to write down anything extra beyond the diaper changes, feedings and sleeping.',\n",
              " 'I like that it has an area to write down medications.',\n",
              " \"It is a great reminder for dr appointments and to track your baby's patterns.\",\n",
              " 'I am extremely happy with this book and will be ordering another one for when I run out of pages in 3 months.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this instance both tokenizers split the sentences similarly. <br><br>\n",
        "\n",
        "\n",
        "### 1.3 Word Tokenization\n",
        "\n",
        "We can further tokenize our documents into even more granular tokens. Let's look at examples of word tokenizers in word_tokenize and wordpunct_tokenize methods from nltk."
      ],
      "metadata": {
        "id": "IE7kSWKgS6-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
        "\n",
        "print(\"Word Tokenizer: \", word_tokenize(corpus[0])[:10])\n",
        "print(\"Word Punct Tokenizer: \", wordpunct_tokenize(corpus[0])[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ktqSt19S35z",
        "outputId": "aabc073c-134f-4793-d415-bd27d87fef78"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenizer:  ['All', 'of', 'my', 'kids', 'have', 'cried', 'non-stop', 'when', 'I', 'tried']\n",
            "Word Punct Tokenizer:  ['All', 'of', 'my', 'kids', 'have', 'cried', 'non', '-', 'stop', 'when']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe that the result is a list of individual words extracted from the document. The output above displays the first 10 words generated by both word tokenizers. It is worth noting that the word_tokenize function does not separate the word 'non-stop' into distinct tokens, whereas the wordpunct_tokenize function does split it into separate tokens"
      ],
      "metadata": {
        "id": "QACT0B02TJJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Stemming\n",
        "Stemming is the process of reducing texts to their root words. This feature is powerful as it allows us to reduce the number of tokens in a document or corpus significantly while retaining the context as much as possible. In the example below, we implement two stemmers on the set of variants with the same root.\n",
        "\n",
        "Let's see the examples of stemmers in nltk"
      ],
      "metadata": {
        "id": "nHqmfrYLTLdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "\n",
        "porter_stem = PorterStemmer()\n",
        "snowb_stem = SnowballStemmer('english') # Need to initialize "
      ],
      "metadata": {
        "id": "pUs2IE_XTGJG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter_stem.stem(\"loved\"), porter_stem.stem(\"lovely\"), porter_stem.stem(\"loveness\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNWFdqkGTR74",
        "outputId": "b8aa85cd-a1b6-4f01-fd5d-2b15782473d3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('love', 'love', 'love')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snowb_stem.stem('amazing'), snowb_stem.stem('amazed'), snowb_stem.stem('amazes'), snowb_stem.stem('amazingly')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0l_xj7nTTn4",
        "outputId": "110a7474-2cd4-44df-c2d4-07a4ef21ce7c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('amaz', 'amaz', 'amaz', 'amaz')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Lemmatization\n",
        "\n",
        "Lemmatization also performs stemming however, it uses vocabulary and morphology to return the base of the word."
      ],
      "metadata": {
        "id": "_ss6ZpwxTXub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatizer.lemmatize(\"wolves\"), lemmatizer.lemmatize(\"saying\"), lemmatizer.lemmatize(\"is\", pos='v')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTt4OvOHTVHp",
        "outputId": "a64dd2d4-1120-451e-ad78-8b2b2119789c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('wolf', 'saying', 'be')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Stop Words\n",
        "\n",
        "Stopwords are words that are part of the grammatical structure of the language but do not carry much semantic meaning to the text. These are often frequent words like 'is' and 'the' that whether they are in or out of the text, the meaning does not change. In NLP, we often deal with stopwords by removing them or selecting what to retain.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZxbnjT6XTdb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sentence = \"the restaurant at the city served amazing food\"\n",
        "print(\"With Stop words:\", sentence, \"\\n\" )\n",
        "\n",
        "without_stopwords = [word for word in word_tokenize(sentence) if word not in stopwords.words('english')]\n",
        "print('Without Stop words:', ' '.join(without_stopwords))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fgkuIc3TbOe",
        "outputId": "59586c05-4506-457d-9f63-76a6833dac1a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With Stop words: the restaurant at the city served amazing food \n",
            "\n",
            "Without Stop words: restaurant city served amazing food\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combining Preprocessing Steps\n",
        "\n",
        "\n",
        "We have discussed concepts and techniques to preprocess data for feature extraction. The next step is to put them together into a preprocessing function that can be run against the dataset. Below is an example of one such function."
      ],
      "metadata": {
        "id": "y1CKiKF0Tnst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "6vywomVEThUz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "def cleaningText(text):\n",
        "    \"\"\"\n",
        "    Text Cleaning:\n",
        "        - Remove Punctuation\n",
        "        - Remove Numbers\n",
        "        - Tokenize Text\n",
        "        - Stem Text\n",
        "        - Remove Stopwords\n",
        "    \"\"\" \n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text) # Remove Punctuation\n",
        "    text = re.sub(\"[0-9]+\", \"\", text) # Remove Numbers\n",
        "    text = [ porter_stemmer.stem(word.lower()) for word in word_tokenize(text) if word not in stopwords.words('english') ]\n",
        "    return \" \".join(text)"
      ],
      "metadata": {
        "id": "-SEpvod-TuEI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaningText(\"The restaurantant has really amazing service and great food\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZliizunpT0NK",
        "outputId": "0baf9792-736a-4b86-e96a-423e76b423cb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the restaurant realli amaz servic great food'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have learned the basics, we can go ahead and use these preprocessing to clean the data and then extract features from the data. In Part 2, we build on these concepts to explore feature extraction techniques."
      ],
      "metadata": {
        "id": "6Aq9YR4kT3uk"
      }
    }
  ]
}